% =============================================================================
% Section 9: Introduction to Neural Networks
% What are neural networks and why do they work?
% =============================================================================

\section{Introduction to Neural Networks}

% -----------------------------------------------------------------------------
% Opening
% -----------------------------------------------------------------------------
\begin{frame}{Neural Networks: Inspired by Biology}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{funnybox}
                \textit{``Humans tried to build flying machines by copying birds. It kinda worked. Then we tried copying brains. It... also kinda worked?''}
            \end{funnybox}
            
            \vspace{0.3cm}
            
            \textbf{The Big Idea:}
            \begin{itemize}
                \item Brains are amazing at learning
                \item Brains are made of neurons
                \item Let's build artificial neurons!
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \begin{tikzpicture}[scale=0.9]
                % Biological neuron (simplified)
                % Cell body
                \draw[thick,accentcyan,fill=darkgray] (0,0) circle (0.5);
                \node[accentcyan,font=\tiny] at (0,0) {soma};
                
                % Dendrites
                \draw[thick,neongreen] (-1.5,0.5) -- (-0.5,0.2);
                \draw[thick,neongreen] (-1.5,0) -- (-0.5,0);
                \draw[thick,neongreen] (-1.5,-0.5) -- (-0.5,-0.2);
                \node[neongreen,font=\tiny,left] at (-1.5,0.5) {dendrites};
                
                % Axon
                \draw[thick,neonpink] (0.5,0) -- (2,0);
                \draw[thick,neonpink] (2,0) -- (2.5,0.3);
                \draw[thick,neonpink] (2,0) -- (2.5,0);
                \draw[thick,neonpink] (2,0) -- (2.5,-0.3);
                \node[neonpink,font=\tiny,above] at (1.25,0) {axon};
                
                % Arrow
                \draw[->,thick,neonyellow] (0,-1) -- (0,-1.8);
                
                % Artificial neuron
                \node[circle,draw=accentcyan,fill=darkgray,minimum size=1cm] (n) at (0,-2.8) {$\sigma$};
                \draw[thick,neongreen] (-1.5,-2.5) -- (-0.5,-2.7);
                \draw[thick,neongreen] (-1.5,-2.8) -- (-0.5,-2.8);
                \draw[thick,neongreen] (-1.5,-3.1) -- (-0.5,-2.9);
                \draw[->,thick,neonpink] (0.5,-2.8) -- (1.5,-2.8);
                
                \node[fgwhite,font=\tiny] at (0,-3.7) {Artificial Neuron};
            \end{tikzpicture}
        \end{column}
    \end{columns}
\end{frame}

% -----------------------------------------------------------------------------
% The Perceptron
% -----------------------------------------------------------------------------
\begin{frame}{The Perceptron: Simplest Neural Unit}
    \begin{defbox}[Perceptron (Rosenblatt, 1958)]
        A \glow{perceptron} computes:
        \[
            y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right) = \sigma(\mathbf{w}^\top \mathbf{x} + b)
        \]
        
        Where:
        \begin{itemize}
            \item $\mathbf{x} = (x_1, \ldots, x_n)$: inputs
            \item $\mathbf{w} = (w_1, \ldots, w_n)$: weights
            \item $b$: bias
            \item $\sigma$: activation function
        \end{itemize}
    \end{defbox}
    
    \centering
    \begin{tikzpicture}[scale=0.7]
        % Inputs
        \node[input neuron] (x1) at (0,1.5) {$x_1$};
        \node[input neuron] (x2) at (0,0) {$x_2$};
        \node[input neuron] (x3) at (0,-1.5) {$x_3$};
        
        % Neuron
        \node[hidden neuron,minimum size=1cm] (n) at (3,0) {$\sigma$};
        
        % Output
        \node[output neuron] (y) at (6,0) {$y$};
        
        % Connections with weights
        \draw[connection] (x1) -- (n) node[midway,above,font=\small] {$w_1$};
        \draw[connection] (x2) -- (n) node[midway,above,font=\small] {$w_2$};
        \draw[connection] (x3) -- (n) node[midway,below,font=\small] {$w_3$};
        \draw[connection] (n) -- (y);
        
        % Bias
        \node[below,neonyellow,font=\small] at (3,-1.2) {$+b$};
    \end{tikzpicture}
\end{frame}

% -----------------------------------------------------------------------------
% Linear Combination
% -----------------------------------------------------------------------------
\begin{frame}{Step 1: Weighted Sum (Linear Combination)}
    \begin{defbox}[Weighted Sum]
        The neuron computes a weighted sum of inputs:
        \[
            z = \sum_{i=1}^{n} w_i x_i + b = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
        \]
    \end{defbox}
    
    \vspace{0.3cm}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Example:}
            \begin{align*}
                \mathbf{x} &= (0.5, 0.8, -0.3) \\
                \mathbf{w} &= (0.4, -0.2, 0.6) \\
                b &= 0.1
            \end{align*}
            
            \begin{align*}
                z &= 0.4(0.5) + (-0.2)(0.8) + 0.6(-0.3) + 0.1 \\
                &= 0.2 - 0.16 - 0.18 + 0.1 \\
                &= -0.04
            \end{align*}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{funnybox}
                Weighted sum = ``voting with different levels of influence''
                
                \vspace{0.1cm}
                
                Big $|w_i|$ = strong opinion\\
                $w_i > 0$ = votes YES\\
                $w_i < 0$ = votes NO
            \end{funnybox}
        \end{column}
    \end{columns}
\end{frame}

% -----------------------------------------------------------------------------
% Why Activation Functions?
% -----------------------------------------------------------------------------
\begin{frame}{Step 2: Activation Function â€” Adding Non-Linearity}
    \begin{alertbox}[The Problem]
        Linear functions composed = still linear!
        \[
            f_2(f_1(\mathbf{x})) = W_2(W_1\mathbf{x} + b_1) + b_2 = W_2 W_1 \mathbf{x} + (W_2 b_1 + b_2) = W'\mathbf{x} + b'
        \]
    \end{alertbox}
    
    \vspace{0.3cm}
    
    \begin{keybox}[Solution: Non-Linear Activation]
        Apply a non-linear function $\sigma$ after each linear transform:
        \[
            y = \sigma(z) = \sigma(\mathbf{w}^\top \mathbf{x} + b)
        \]
    \end{keybox}
    
    \vspace{0.3cm}
    
    \begin{funnybox}
        Without activation functions, a 1000-layer network is just... a single matrix multiplication wearing a disguise.
    \end{funnybox}
\end{frame}

% -----------------------------------------------------------------------------
% Common Activation Functions
% -----------------------------------------------------------------------------
\begin{frame}{Common Activation Functions}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{defbox}[Sigmoid]
                \[
                    \sigma(z) = \frac{1}{1 + e^{-z}}
                \]
                
                Range: $(0, 1)$
                
                \centering
                \begin{tikzpicture}[scale=0.5]
                    \draw[->] (-3,0) -- (3,0) node[right] {$z$};
                    \draw[->] (0,-0.5) -- (0,2) node[above] {$\sigma$};
                    \draw[thick,neonpink,domain=-2.5:2.5,samples=50] plot (\x, {1.5/(1+exp(-\x))});
                    \draw[dashed,fgwhite] (-2.5,1.5) -- (2.5,1.5);
                \end{tikzpicture}
            \end{defbox}
            
            \begin{defbox}[Tanh]
                \[
                    \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
                \]
                
                Range: $(-1, 1)$
            \end{defbox}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{defbox}[ReLU (Most Popular!)]
                \[
                    \text{ReLU}(z) = \max(0, z)
                \]
                
                Range: $[0, \infty)$
                
                \centering
                \begin{tikzpicture}[scale=0.5]
                    \draw[->] (-2,0) -- (3,0) node[right] {$z$};
                    \draw[->] (0,-0.5) -- (0,2.5) node[above] {ReLU};
                    \draw[thick,neongreen] (-2,0) -- (0,0) -- (2,2);
                \end{tikzpicture}
            \end{defbox}
            
            \begin{keybox}
                ReLU is fast, simple, and works!
                
                \textit{``If in doubt, use ReLU.''}
            \end{keybox}
        \end{column}
    \end{columns}
\end{frame}

% -----------------------------------------------------------------------------
% Universal Approximation
% -----------------------------------------------------------------------------
\begin{frame}{The Universal Approximation Theorem}
    \begin{thmbox}[Universal Approximation (Cybenko, 1989)]
        A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\mathbb{R}^n$ to arbitrary accuracy.
    \end{thmbox}
    
    \vspace{0.3cm}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{funnybox}
                \textit{``Given enough neurons, a neural network can approximate ANY function!''}
                
                \vspace{0.1cm}
                
                The catch? ``Enough'' might be astronomically large.
            \end{funnybox}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{infobox}[Implications]
                \begin{itemize}
                    \item NNs are \textit{universal function approximators}
                    \item Existence theorem (not constructive)
                    \item Depth helps: deeper = more efficient
                \end{itemize}
            \end{infobox}
        \end{column}
    \end{columns}
\end{frame}

% -----------------------------------------------------------------------------
% Learning as Optimization
% -----------------------------------------------------------------------------
\begin{frame}{Learning = Finding Good Weights}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{defbox}[The Learning Problem]
                Given:
                \begin{itemize}
                    \item Data: $\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^{N}$
                    \item Network architecture
                \end{itemize}
                
                Find: weights $\mathbf{W}$ such that
                \[
                    f_{\mathbf{W}}(\mathbf{x}^{(i)}) \approx y^{(i)} \quad \forall i
                \]
            \end{defbox}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \begin{tikzpicture}[scale=0.8]
                % Loss landscape (simplified)
                \draw[->] (0,0) -- (4,0) node[right,font=\small] {$w$};
                \draw[->] (0,0) -- (0,3) node[above,font=\small] {Loss};
                
                % Curve
                \draw[thick,accentcyan,domain=0.5:3.5,samples=50] 
                    plot (\x, {0.5 + 1.5*(\x-2)*(\x-2)});
                
                % Minimum
                \fill[neongreen] (2,0.5) circle (2pt);
                \node[below,neongreen,font=\small] at (2,0.3) {optimal $w^*$};
                
                % Current point
                \fill[neonpink] (0.8,1.72) circle (2pt);
                \node[above,neonpink,font=\small] at (0.8,1.72) {current $w$};
                
                % Arrow
                \draw[->,thick,neonyellow] (0.8,1.72) -- (1.4,1.1);
            \end{tikzpicture}
            
            \textit{Minimize loss by adjusting weights}
        \end{column}
    \end{columns}
    
    \vspace{0.3cm}
    
    \begin{keybox}
        \textbf{Training} = searching for weights that minimize prediction error
        
        Tool: \textbf{Gradient Descent} (uses derivatives we learned!)
    \end{keybox}
\end{frame}

% -----------------------------------------------------------------------------
% What Can NNs Do?
% -----------------------------------------------------------------------------
\begin{frame}{What Can Neural Networks Do?}
    \begin{columns}[T]
        \begin{column}{0.33\textwidth}
            \begin{keybox}[Classification]
                Is this a cat or dog?
                
                \centering
                \begin{tikzpicture}[scale=0.5]
                    \draw[thick,accentcyan] (0,0) circle (1);
                    \draw[thick,neonpink] (0,0) ellipse (2 and 1.5);
                    \fill[neongreen] (0.3,0.3) circle (2pt);
                    \fill[neonyellow] (-1.2,0.5) circle (2pt);
                \end{tikzpicture}
                
                \textit{Separate classes}
            \end{keybox}
        \end{column}
        \begin{column}{0.33\textwidth}
            \begin{keybox}[Regression]
                Predict house price
                
                \centering
                \begin{tikzpicture}[scale=0.5]
                    \draw[->] (0,0) -- (2.5,0);
                    \draw[->] (0,0) -- (0,2);
                    \draw[thick,neonpink,domain=0:2,samples=30] plot (\x, {0.5 + 0.6*\x});
                    \foreach \x/\y in {0.3/0.7, 0.8/1.1, 1.2/0.9, 1.6/1.4, 2/1.5} {
                        \fill[accentcyan] (\x,\y) circle (2pt);
                    }
                \end{tikzpicture}
                
                \textit{Continuous output}
            \end{keybox}
        \end{column}
        \begin{column}{0.33\textwidth}
            \begin{keybox}[Generation]
                Create new images/text
                
                \centering
                \begin{tikzpicture}[scale=0.5]
                    \draw[thick,accentcyan,rounded corners] (0,0) rectangle (2,1.5);
                    \node[font=\Large] at (1,0.75) {?};
                    \draw[->,thick,neongreen] (2.2,0.75) -- (2.8,0.75);
                    \draw[thick,neonpink,rounded corners] (3,0) rectangle (5,1.5);
                    \node[font=\Large,neonpink] at (4,0.75) {Art};
                \end{tikzpicture}
                
                \textit{Create content}
            \end{keybox}
        \end{column}
    \end{columns}
    
    \vspace{0.5cm}
    
    \begin{funnybox}
        Neural networks power: image recognition, language translation, game playing, drug discovery, art generation, and... whatever GPT does when nobody's looking.
    \end{funnybox}
\end{frame}

% -----------------------------------------------------------------------------
% Key Takeaways
% -----------------------------------------------------------------------------
\begin{frame}{Key Takeaways: Neural Networks Introduction}
    \begin{keybox}
        \begin{enumerate}
            \item A \textbf{neuron} computes: $y = \sigma(\mathbf{w}^\top \mathbf{x} + b)$
            \item Components:
            \begin{itemize}
                \item \textbf{Weights} ($\mathbf{w}$): learned parameters
                \item \textbf{Bias} ($b$): threshold/offset
                \item \textbf{Activation} ($\sigma$): adds non-linearity
            \end{itemize}
            \item \textbf{ReLU} is the most common activation: $\max(0, z)$
            \item \textbf{Universal Approximation}: NNs can approximate any function
            \item \textbf{Learning} = finding weights that minimize error
            \item Applications: classification, regression, generation
        \end{enumerate}
    \end{keybox}
    
    \vspace{0.2cm}
    
    \centering
    \textit{Next: Building deeper networks with multiple layers!}
\end{frame}
