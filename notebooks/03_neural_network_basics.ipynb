{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73efa839",
   "metadata": {},
   "source": [
    "# üß† ML Workshop - Neural Network Basics\n",
    "\n",
    "**Author:** Shuvam Banerji Seal\n",
    "\n",
    "This notebook covers:\n",
    "- Perceptrons and neurons\n",
    "- Activation functions\n",
    "- Multi-layer perceptrons (MLPs)\n",
    "- Forward propagation\n",
    "- Building a simple neural network from scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrow, FancyBboxPatch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set style\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Colors\n",
    "CYAN = '#00d9ff'\n",
    "PINK = '#ff6b9d'\n",
    "YELLOW = '#c8ff00'\n",
    "GREEN = '#00ff88'\n",
    "ORANGE = '#ff9500'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dda0e2",
   "metadata": {},
   "source": [
    "## 1. The Perceptron: Simplest Neural Unit\n",
    "\n",
    "$$y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b) = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n",
    "\n",
    "Where $\\sigma$ is an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single neuron\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Draw neuron body\n",
    "neuron = Circle((0.5, 0.5), 0.1, color=CYAN, ec='white', linewidth=2, zorder=10)\n",
    "ax.add_patch(neuron)\n",
    "\n",
    "# Draw inputs\n",
    "input_labels = ['$x_1$', '$x_2$', '$x_3$', '$b$ (bias)']\n",
    "input_y = [0.8, 0.6, 0.4, 0.2]\n",
    "weight_labels = ['$w_1$', '$w_2$', '$w_3$', '1']\n",
    "colors = [PINK, YELLOW, GREEN, ORANGE]\n",
    "\n",
    "for i, (label, y, w_label, color) in enumerate(zip(input_labels, input_y, weight_labels, colors)):\n",
    "    # Input circles\n",
    "    input_circle = Circle((0.1, y), 0.03, color=color, ec='white', zorder=10)\n",
    "    ax.add_patch(input_circle)\n",
    "    ax.text(0.03, y, label, fontsize=14, ha='center', va='center', color=color)\n",
    "    \n",
    "    # Arrows with weights\n",
    "    ax.annotate('', xy=(0.4, 0.5), xytext=(0.13, y),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=2))\n",
    "    ax.text(0.25, (y + 0.5)/2 + 0.03, w_label, fontsize=12, color=color)\n",
    "\n",
    "# Output\n",
    "ax.annotate('', xy=(0.75, 0.5), xytext=(0.6, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='white', lw=2))\n",
    "ax.text(0.85, 0.5, '$y = \\sigma(\\sum w_i x_i + b)$', fontsize=14, va='center')\n",
    "\n",
    "# Labels\n",
    "ax.text(0.5, 0.5, '$\\Sigma$', fontsize=20, ha='center', va='center', color='black', zorder=11)\n",
    "ax.text(0.5, 0.32, 'Neuron', fontsize=12, ha='center', color=CYAN)\n",
    "\n",
    "# Box for formula\n",
    "formula_box = FancyBboxPatch((0.6, 0.1), 0.38, 0.15, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='#1a1a2e', edgecolor=CYAN, linewidth=2)\n",
    "ax.add_patch(formula_box)\n",
    "ax.text(0.79, 0.175, 'Forward Pass:\\n$z = w_1x_1 + w_2x_2 + w_3x_3 + b$\\n$y = \\sigma(z)$', \n",
    "        fontsize=11, ha='center', va='center', color='white')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Anatomy of a Single Neuron', fontsize=16, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed098b",
   "metadata": {},
   "source": [
    "## 2. Activation Functions: Adding Non-linearity\n",
    "\n",
    "Without activation functions, a neural network is just a linear transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "# Plot all activation functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "activations = [\n",
    "    (sigmoid, 'Sigmoid: $\\sigma(x) = \\\\frac{1}{1+e^{-x}}$', CYAN, 'Outputs (0,1), good for probabilities'),\n",
    "    (relu, 'ReLU: $f(x) = \\max(0, x)$', PINK, 'Most popular! Fast, sparse activations'),\n",
    "    (tanh, 'Tanh: $f(x) = \\\\tanh(x)$', YELLOW, 'Outputs (-1,1), zero-centered'),\n",
    "    (leaky_relu, 'Leaky ReLU: $f(x) = \\max(0.01x, x)$', GREEN, 'Fixes \"dying ReLU\" problem'),\n",
    "]\n",
    "\n",
    "for ax, (func, title, color, desc) in zip(axes.flatten(), activations):\n",
    "    y = func(x)\n",
    "    ax.plot(x, y, color=color, linewidth=3)\n",
    "    ax.axhline(y=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(x=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax.set_xlabel('z (pre-activation)')\n",
    "    ax.set_ylabel('a (activation)')\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add description box\n",
    "    ax.text(0.02, 0.98, desc, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='#1a1a2e', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Activation Functions', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174b46e",
   "metadata": {},
   "source": [
    "## 3. Build a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3121bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network built from scratch.\n",
    "    Architecture: Input -> Hidden (ReLU) -> Output (Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        print(f\"\\nüß† Neural Network Created:\")\n",
    "        print(f\"   Input layer:  {input_size} neurons\")\n",
    "        print(f\"   Hidden layer: {hidden_size} neurons (ReLU)\")\n",
    "        print(f\"   Output layer: {output_size} neurons (Sigmoid)\")\n",
    "        print(f\"   Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return (self.W1.size + self.b1.size + self.W2.size + self.b2.size)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.1):\n",
    "        \"\"\"Backpropagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = (1/m) * self.a1.T @ dz2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = (dz2 @ self.W2.T) * self.relu_derivative(self.z1)\n",
    "        dW1 = (1/m) * X.T @ dz1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=100, learning_rate=0.1, verbose=True):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if verbose and epoch % (epochs // 10) == 0:\n",
    "                acc = np.mean((y_pred > 0.5) == y) * 100\n",
    "                print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Accuracy = {acc:.1f}%\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37505976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset (classic non-linear problem)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate noisy XOR data\n",
    "n_samples = 200\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = ((X[:, 0] > 0) ^ (X[:, 1] > 0)).astype(float).reshape(-1, 1)\n",
    "\n",
    "# Add some noise\n",
    "X += np.random.randn(n_samples, 2) * 0.1\n",
    "\n",
    "# Visualize data\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='coolwarm', s=50, edgecolors='white')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('XOR Dataset (Non-linear!)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "ax.axvline(x=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   Samples: {n_samples}\")\n",
    "print(f\"   Features: 2\")\n",
    "print(f\"   Classes: 2 (binary)\")\n",
    "print(f\"   XOR: A linear model CANNOT solve this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=8, output_size=1)\n",
    "losses = nn.train(X, y, epochs=1000, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, color=CYAN, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (Binary Cross-Entropy)')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "ax = axes[1]\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = nn.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, levels=[0, 0.5, 1], cmap='coolwarm', alpha=0.4)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors=['white'], linewidths=[2])\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='coolwarm', s=50, edgecolors='white')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Learned Decision Boundary')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final accuracy\n",
    "predictions = nn.predict(X)\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "print(f\"\\nüéØ Final Accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584b492",
   "metadata": {},
   "source": [
    "## 4. Visualize Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_network(layer_sizes, ax, title='Neural Network'):\n",
    "    \"\"\"\n",
    "    Draw a neural network diagram.\n",
    "    \"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    max_neurons = max(layer_sizes)\n",
    "    \n",
    "    layer_colors = [GREEN, CYAN, YELLOW, PINK, ORANGE]\n",
    "    if n_layers > len(layer_colors):\n",
    "        layer_colors = layer_colors * (n_layers // len(layer_colors) + 1)\n",
    "    \n",
    "    # Calculate positions\n",
    "    v_spacing = 1.0 / max_neurons\n",
    "    h_spacing = 1.0 / (n_layers + 1)\n",
    "    \n",
    "    # Store neuron positions for connections\n",
    "    positions = []\n",
    "    \n",
    "    for layer_idx, n_neurons in enumerate(layer_sizes):\n",
    "        layer_positions = []\n",
    "        x = (layer_idx + 1) * h_spacing\n",
    "        \n",
    "        # Center neurons vertically\n",
    "        start_y = 0.5 - (n_neurons - 1) * v_spacing / 2\n",
    "        \n",
    "        for neuron_idx in range(n_neurons):\n",
    "            y = start_y + neuron_idx * v_spacing\n",
    "            layer_positions.append((x, y))\n",
    "            \n",
    "            # Draw neuron\n",
    "            circle = Circle((x, y), 0.02, color=layer_colors[layer_idx], \n",
    "                           ec='white', linewidth=1.5, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        positions.append(layer_positions)\n",
    "    \n",
    "    # Draw connections\n",
    "    for layer_idx in range(n_layers - 1):\n",
    "        for start_pos in positions[layer_idx]:\n",
    "            for end_pos in positions[layer_idx + 1]:\n",
    "                ax.plot([start_pos[0], end_pos[0]], [start_pos[1], end_pos[1]], \n",
    "                       'w-', alpha=0.2, linewidth=0.5, zorder=1)\n",
    "    \n",
    "    # Add layer labels\n",
    "    labels = ['Input'] + [f'Hidden {i+1}' for i in range(n_layers - 2)] + ['Output']\n",
    "    for i, (label, n_neurons) in enumerate(zip(labels, layer_sizes)):\n",
    "        x = (i + 1) * h_spacing\n",
    "        ax.text(x, 0.05, f'{label}\\n({n_neurons})', ha='center', fontsize=10, color='white')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "\n",
    "\n",
    "# Visualize different architectures\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "architectures = [\n",
    "    ([2, 4, 1], 'Simple: 2 ‚Üí 4 ‚Üí 1'),\n",
    "    ([4, 8, 8, 2], 'Deeper: 4 ‚Üí 8 ‚Üí 8 ‚Üí 2'),\n",
    "    ([784, 256, 128, 10], 'MNIST: 784 ‚Üí 256 ‚Üí 128 ‚Üí 10'),\n",
    "]\n",
    "\n",
    "for ax, (layers, title) in zip(axes, architectures):\n",
    "    # For large networks, show representative neurons\n",
    "    display_layers = [min(l, 12) for l in layers]\n",
    "    draw_neural_network(display_layers, ax, title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3027c3",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Walkthrough\n",
    "\n",
    "Let's trace through a forward pass with real numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny network for demonstration\n",
    "np.random.seed(0)\n",
    "\n",
    "# Simple weights\n",
    "W1 = np.array([[0.5, -0.3], \n",
    "               [0.2, 0.8]])\n",
    "b1 = np.array([[0.1, -0.1]])\n",
    "\n",
    "W2 = np.array([[0.4], \n",
    "               [-0.5]])\n",
    "b2 = np.array([[0.2]])\n",
    "\n",
    "# Input\n",
    "x = np.array([[1.0, 2.0]])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD PASS WALKTHROUGH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüì• INPUT: x = {x}\")\n",
    "print(f\"\\nüìä WEIGHTS:\")\n",
    "print(f\"   W1 = \\n{W1}\")\n",
    "print(f\"   b1 = {b1}\")\n",
    "print(f\"   W2 = {W2.T}\")\n",
    "print(f\"   b2 = {b2}\")\n",
    "\n",
    "# Layer 1: Linear transformation\n",
    "z1 = x @ W1 + b1\n",
    "print(f\"\\nüî¢ LAYER 1 - Linear:\")\n",
    "print(f\"   z1 = x @ W1 + b1\")\n",
    "print(f\"   z1 = {x} @ \\n{W1}\")\n",
    "print(f\"      + {b1}\")\n",
    "print(f\"   z1 = {z1}\")\n",
    "\n",
    "# Layer 1: Activation (ReLU)\n",
    "a1 = np.maximum(0, z1)\n",
    "print(f\"\\n‚ö° LAYER 1 - ReLU Activation:\")\n",
    "print(f\"   a1 = max(0, z1)\")\n",
    "print(f\"   a1 = {a1}\")\n",
    "\n",
    "# Layer 2: Linear transformation\n",
    "z2 = a1 @ W2 + b2\n",
    "print(f\"\\nüî¢ LAYER 2 - Linear:\")\n",
    "print(f\"   z2 = a1 @ W2 + b2\")\n",
    "print(f\"   z2 = {a1} @ {W2.T} + {b2}\")\n",
    "print(f\"   z2 = {z2}\")\n",
    "\n",
    "# Layer 2: Activation (Sigmoid)\n",
    "a2 = 1 / (1 + np.exp(-z2))\n",
    "print(f\"\\n‚ö° LAYER 2 - Sigmoid Activation:\")\n",
    "print(f\"   a2 = sigmoid(z2) = 1/(1 + e^(-z2))\")\n",
    "print(f\"   a2 = {a2}\")\n",
    "\n",
    "print(f\"\\nüì§ OUTPUT: y_pred = {a2[0,0]:.4f}\")\n",
    "print(f\"   ‚Üí Predicted class: {1 if a2[0,0] > 0.5 else 0}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd0d69",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Perceptron**: The basic unit of neural networks (weighted sum + activation)\n",
    "2. **Activation Functions**: ReLU, Sigmoid, Tanh ‚Äî adding non-linearity\n",
    "3. **Built a Neural Network from Scratch**: Implemented forward pass, loss, and backprop\n",
    "4. **XOR Problem**: Demonstrated that NNs can learn non-linear boundaries\n",
    "5. **Forward Pass**: Traced through computation with real numbers\n",
    "\n",
    "**Key Insights:**\n",
    "- Neural networks are just compositions of linear transformations + activations\n",
    "- Non-linearity (activation functions) is what makes NNs powerful\n",
    "- Backpropagation is just the chain rule applied systematically\n",
    "\n",
    "**Next**: Training a network on MNIST!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
